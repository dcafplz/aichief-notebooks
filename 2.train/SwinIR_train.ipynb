{"cells":[{"cell_type":"markdown","metadata":{"id":"mxCWWcG6LZ3A"},"source":["### Ready"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"08VuItV9khDt","outputId":"979870cb-7068-4a93-ae2a-b57ca3bf1321","executionInfo":{"status":"ok","timestamp":1667887717265,"user_tz":-540,"elapsed":10563,"user":{"displayName":"ᄆᄆ","userId":"04446057954788773347"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n","\u001b[K     |████████████████████████████████| 548 kB 4.8 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 47.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->timm) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.13.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.10.1 timm-0.6.11\n"]}],"source":["!pip install timm # Installing timm, a computer vision library for pytorch\n","# Downloading a swin IR Model.\n","# More models can be found here: https://github.com/JingyunLiang/SwinIR/releases/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DIEFUhqKk6P5"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import os\n","import cv2\n","import numpy as np\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","\n","from tqdm.auto import tqdm\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import torchvision.models as models\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import random\n","import shutil\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2fhs9SYPsER"},"outputs":[],"source":["CFG = {\n","    'IMG_SIZE':16,\n","    'EPOCHS':1,\n","    'LEARNING_RATE':0.001,\n","    'BATCH_SIZE':650,\n","    'SEED':0,\n","    'SCALE':4,\n","    'DEVICE':'cuda' if torch.cuda.is_available() else 'cpu' \n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_71ZvghaPxYL"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(CFG['SEED']) # Seed 고정"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-PYGLbN2kDSD","outputId":"b405c9b9-d3c2-47fb-e4ec-6cdc04423f6d","executionInfo":{"status":"ok","timestamp":1667887741794,"user_tz":-540,"elapsed":19476,"user":{"displayName":"ᄆᄆ","userId":"04446057954788773347"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"IhQlqb3Q_x7P"},"source":["### define model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-2Ukou2cjs5d"},"outputs":[],"source":["# We need the code for the SwinIR Model. Copying that file will give us a Pytorch model called SwinIR\n","# We can load the weights that we downloaded above into the SwinIR Object\n","\n","import math\n","import torch.utils.checkpoint as checkpoint\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size\n","        H (int): Height of image\n","        W (int): Width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n","\n","    def flops(self, N):\n","        # calculate flops for 1 window with token length of N\n","        flops = 0\n","        # qkv = self.qkv(x)\n","        flops += N * self.dim * 3 * self.dim\n","        # attn = (q @ k.transpose(-2, -1))\n","        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n","        #  x = (attn @ v)\n","        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n","        # x = self.proj(x)\n","        flops += N * self.dim * self.dim\n","        return flops\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resulotion.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        if min(self.input_resolution) <= self.window_size:\n","            # if window size is larger than input resolution, we don't partition windows\n","            self.shift_size = 0\n","            self.window_size = min(self.input_resolution)\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if self.shift_size > 0:\n","            attn_mask = self.calculate_mask(self.input_resolution)\n","        else:\n","            attn_mask = None\n","\n","        self.register_buffer(\"attn_mask\", attn_mask)\n","\n","    def calculate_mask(self, x_size):\n","        # calculate attention mask for SW-MSA\n","        H, W = x_size\n","        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n","        h_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        w_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","        return attn_mask\n","\n","    def forward(self, x, x_size):\n","        H, W = x_size\n","        B, L, C = x.shape\n","        # assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n","\n","        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n","        if self.input_resolution == x_size:\n","            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n","        else:\n","            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n","               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.input_resolution\n","        # norm1\n","        flops += self.dim * H * W\n","        # W-MSA/SW-MSA\n","        nW = H * W / self.window_size / self.window_size\n","        flops += nW * self.attn.flops(self.window_size * self.window_size)\n","        # mlp\n","        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n","        # norm2\n","        flops += self.dim * H * W\n","        return flops\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.dim\n","        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n","        return flops\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\" A basic Swin Transformer layer for one stage.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n","                                 num_heads=num_heads, window_size=window_size,\n","                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                                 mlp_ratio=mlp_ratio,\n","                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                 drop=drop, attn_drop=attn_drop,\n","                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                                 norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","        # patch merging layer\n","        if downsample is not None:\n","            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def forward(self, x, x_size):\n","        for blk in self.blocks:\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x, x_size)\n","            else:\n","                x = blk(x, x_size)\n","        if self.downsample is not None:\n","            x = self.downsample(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n","\n","    def flops(self):\n","        flops = 0\n","        for blk in self.blocks:\n","            flops += blk.flops()\n","        if self.downsample is not None:\n","            flops += self.downsample.flops()\n","        return flops\n","\n","\n","class RSTB(nn.Module):\n","    \"\"\"Residual Swin Transformer Block (RSTB).\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","        img_size: Input image size.\n","        patch_size: Patch size.\n","        resi_connection: The convolutional block before residual connection.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n","                 img_size=224, patch_size=4, resi_connection='1conv'):\n","        super(RSTB, self).__init__()\n","\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","\n","        self.residual_group = BasicLayer(dim=dim,\n","                                         input_resolution=input_resolution,\n","                                         depth=depth,\n","                                         num_heads=num_heads,\n","                                         window_size=window_size,\n","                                         mlp_ratio=mlp_ratio,\n","                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                         drop=drop, attn_drop=attn_drop,\n","                                         drop_path=drop_path,\n","                                         norm_layer=norm_layer,\n","                                         downsample=downsample,\n","                                         use_checkpoint=use_checkpoint)\n","\n","        if resi_connection == '1conv':\n","            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n","        elif resi_connection == '3conv':\n","            # to save parameters and memory\n","            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n","                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n","\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n","            norm_layer=None)\n","\n","        self.patch_unembed = PatchUnEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n","            norm_layer=None)\n","\n","    def forward(self, x, x_size):\n","        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n","\n","    def flops(self):\n","        flops = 0\n","        flops += self.residual_group.flops()\n","        H, W = self.input_resolution\n","        flops += H * W * self.dim * self.dim * 9\n","        flops += self.patch_embed.flops()\n","        flops += self.patch_unembed.flops()\n","\n","        return flops\n","\n","\n","class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.img_size\n","        if self.norm is not None:\n","            flops += H * W * self.embed_dim\n","        return flops\n","\n","\n","class PatchUnEmbed(nn.Module):\n","    r\"\"\" Image to Patch Unembedding\n","\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","    def forward(self, x, x_size):\n","        B, HW, C = x.shape\n","        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        return flops\n","\n","\n","class Upsample(nn.Sequential):\n","    \"\"\"Upsample module.\n","\n","    Args:\n","        scale (int): Scale factor. Supported scales: 2^n and 3.\n","        num_feat (int): Channel number of intermediate features.\n","    \"\"\"\n","\n","    def __init__(self, scale, num_feat):\n","        m = []\n","        if (scale & (scale - 1)) == 0:  # scale = 2^n\n","            for _ in range(int(math.log(scale, 2))):\n","                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n","                m.append(nn.PixelShuffle(2))\n","        elif scale == 3:\n","            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n","            m.append(nn.PixelShuffle(3))\n","        else:\n","            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n","        super(Upsample, self).__init__(*m)\n","\n","\n","class UpsampleOneStep(nn.Sequential):\n","    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n","       Used in lightweight SR to save parameters.\n","\n","    Args:\n","        scale (int): Scale factor. Supported scales: 2^n and 3.\n","        num_feat (int): Channel number of intermediate features.\n","\n","    \"\"\"\n","\n","    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n","        self.num_feat = num_feat\n","        self.input_resolution = input_resolution\n","        m = []\n","        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n","        m.append(nn.PixelShuffle(scale))\n","        super(UpsampleOneStep, self).__init__(*m)\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.num_feat * 3 * 9\n","        return flops\n","\n","\n","class SwinIR(nn.Module):\n","    r\"\"\" SwinIR\n","        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n","\n","    Args:\n","        img_size (int | tuple(int)): Input image size. Default 64\n","        patch_size (int | tuple(int)): Patch size. Default: 1\n","        in_chans (int): Number of input image channels. Default: 3\n","        embed_dim (int): Patch embedding dimension. Default: 96\n","        depths (tuple(int)): Depth of each Swin Transformer layer.\n","        num_heads (tuple(int)): Number of attention heads in different layers.\n","        window_size (int): Window size. Default: 7\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n","        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n","        drop_rate (float): Dropout rate. Default: 0\n","        attn_drop_rate (float): Attention dropout rate. Default: 0\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n","        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n","        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n","        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n","        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n","        img_range: Image range. 1. or 255.\n","        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n","        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n","    \"\"\"\n","\n","    def __init__(self, img_size=64, patch_size=1, in_chans=3,\n","                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],\n","                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n","                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',\n","                 **kwargs):\n","        super(SwinIR, self).__init__()\n","        num_in_ch = in_chans\n","        num_out_ch = in_chans\n","        num_feat = 64\n","        self.img_range = img_range\n","        if in_chans == 3:\n","            rgb_mean = (0.4488, 0.4371, 0.4040)\n","            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n","        else:\n","            self.mean = torch.zeros(1, 1, 1, 1)\n","        self.upscale = upscale\n","        self.upsampler = upsampler\n","        self.window_size = window_size\n","\n","        #####################################################################################################\n","        ################################### 1, shallow feature extraction ###################################\n","        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n","\n","        #####################################################################################################\n","        ################################### 2, deep feature extraction ######################################\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.ape = ape\n","        self.patch_norm = patch_norm\n","        self.num_features = embed_dim\n","        self.mlp_ratio = mlp_ratio\n","\n","        # split image into non-overlapping patches\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","        num_patches = self.patch_embed.num_patches\n","        patches_resolution = self.patch_embed.patches_resolution\n","        self.patches_resolution = patches_resolution\n","\n","        # merge non-overlapping patches into image\n","        self.patch_unembed = PatchUnEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","\n","        # absolute position embedding\n","        if self.ape:\n","            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","            trunc_normal_(self.absolute_pos_embed, std=.02)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","\n","        # build Residual Swin Transformer blocks (RSTB)\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            layer = RSTB(dim=embed_dim,\n","                         input_resolution=(patches_resolution[0],\n","                                           patches_resolution[1]),\n","                         depth=depths[i_layer],\n","                         num_heads=num_heads[i_layer],\n","                         window_size=window_size,\n","                         mlp_ratio=self.mlp_ratio,\n","                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                         drop=drop_rate, attn_drop=attn_drop_rate,\n","                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n","                         norm_layer=norm_layer,\n","                         downsample=None,\n","                         use_checkpoint=use_checkpoint,\n","                         img_size=img_size,\n","                         patch_size=patch_size,\n","                         resi_connection=resi_connection\n","\n","                         )\n","            self.layers.append(layer)\n","        self.norm = norm_layer(self.num_features)\n","\n","        # build the last conv layer in deep feature extraction\n","        if resi_connection == '1conv':\n","            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n","        elif resi_connection == '3conv':\n","            # to save parameters and memory\n","            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n","                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n","                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n","\n","        #####################################################################################################\n","        ################################ 3, high quality image reconstruction ################################\n","        if self.upsampler == 'pixelshuffle':\n","            # for classical SR\n","            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n","                                                      nn.LeakyReLU(inplace=True))\n","            self.upsample = Upsample(upscale, num_feat)\n","            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n","        elif self.upsampler == 'pixelshuffledirect':\n","            # for lightweight SR (to save parameters)\n","            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n","                                            (patches_resolution[0], patches_resolution[1]))\n","        elif self.upsampler == 'nearest+conv':\n","            # for real-world SR (less artifacts)\n","            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n","                                                      nn.LeakyReLU(inplace=True))\n","            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            if self.upscale == 4:\n","                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n","            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        else:\n","            # for image denoising and JPEG compression artifact reduction\n","            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'absolute_pos_embed'}\n","\n","    @torch.jit.ignore\n","    def no_weight_decay_keywords(self):\n","        return {'relative_position_bias_table'}\n","\n","    def check_image_size(self, x):\n","        _, _, h, w = x.size()\n","        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n","        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n","        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n","        return x\n","\n","    def forward_features(self, x):\n","        x_size = (x.shape[2], x.shape[3])\n","        x = self.patch_embed(x)\n","        if self.ape:\n","            x = x + self.absolute_pos_embed\n","        x = self.pos_drop(x)\n","\n","        for layer in self.layers:\n","            x = layer(x, x_size)\n","\n","        x = self.norm(x)  # B L C\n","        x = self.patch_unembed(x, x_size)\n","\n","        return x\n","\n","    def forward(self, x):\n","        H, W = x.shape[2:]\n","        x = self.check_image_size(x)\n","        \n","        self.mean = self.mean.type_as(x)\n","        x = (x - self.mean) * self.img_range\n","\n","        if self.upsampler == 'pixelshuffle':\n","            # for classical SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.conv_before_upsample(x)\n","            x = self.conv_last(self.upsample(x))\n","        elif self.upsampler == 'pixelshuffledirect':\n","            # for lightweight SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.upsample(x)\n","        elif self.upsampler == 'nearest+conv':\n","            # for real-world SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.conv_before_upsample(x)\n","            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n","            if self.upscale == 4:\n","                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n","            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n","        else:\n","            # for image denoising and JPEG compression artifact reduction\n","            x_first = self.conv_first(x)\n","            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n","            x = x + self.conv_last(res)\n","\n","        x = x / self.img_range + self.mean\n","\n","        return x[:, :, :H*self.upscale, :W*self.upscale]\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.patches_resolution\n","        flops += H * W * 3 * self.embed_dim * 9\n","        flops += self.patch_embed.flops()\n","        for i, layer in enumerate(self.layers):\n","            flops += layer.flops()\n","        flops += H * W * 3 * self.embed_dim * self.embed_dim\n","        flops += self.upsample.flops()\n","        return flops"]},{"cell_type":"markdown","metadata":{"id":"SG5XqXvm_3ry"},"source":["### perceptual loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rT4LBCAcVghA"},"outputs":[],"source":["import os\n","import torch\n","from collections import OrderedDict\n","from torch import nn as nn\n","from torchvision.models import vgg as vgg\n","\n","VGG_PRETRAIN_PATH = '/content/drive/MyDrive/final_project/colab/saved_models/vgg19.pth'\n","NAMES = {\n","    'vgg11': [\n","        'conv1_1', 'relu1_1', 'pool1', 'conv2_1', 'relu2_1', 'pool2', 'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2',\n","        'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2',\n","        'pool5'\n","    ],\n","    'vgg13': [\n","        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n","        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'pool4',\n","        'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'pool5'\n","    ],\n","    'vgg16': [\n","        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n","        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'pool3', 'conv4_1', 'relu4_1', 'conv4_2',\n","        'relu4_2', 'conv4_3', 'relu4_3', 'pool4', 'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3',\n","        'pool5'\n","    ],\n","    'vgg19': [\n","        'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2', 'pool1', 'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2', 'pool2',\n","        'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3', 'conv3_4', 'relu3_4', 'pool3', 'conv4_1',\n","        'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3', 'conv4_4', 'relu4_4', 'pool4', 'conv5_1', 'relu5_1',\n","        'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3', 'conv5_4', 'relu5_4', 'pool5'\n","    ]\n","}\n","\n","\n","def insert_bn(names):\n","    \"\"\"Insert bn layer after each conv.\n","    Args:\n","        names (list): The list of layer names.\n","    Returns:\n","        list: The list of layer names with bn layers.\n","    \"\"\"\n","    names_bn = []\n","    for name in names:\n","        names_bn.append(name)\n","        if 'conv' in name:\n","            position = name.replace('conv', '')\n","            names_bn.append('bn' + position)\n","    return names_bn\n","\n","\n","class VGGFeatureExtractor(nn.Module):\n","    \"\"\"VGG network for feature extraction.\n","    In this implementation, we allow users to choose whether use normalization\n","    in the input feature and the type of vgg network. Note that the pretrained\n","    path must fit the vgg type.\n","    Args:\n","        layer_name_list (list[str]): Forward function returns the corresponding\n","            features according to the layer_name_list.\n","            Example: {'relu1_1', 'relu2_1', 'relu3_1'}.\n","        vgg_type (str): Set the type of vgg network. Default: 'vgg19'.\n","        use_input_norm (bool): If True, normalize the input image. Importantly,\n","            the input feature must in the range [0, 1]. Default: True.\n","        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n","            Default: False.\n","        requires_grad (bool): If true, the parameters of VGG network will be\n","            optimized. Default: False.\n","        remove_pooling (bool): If true, the max pooling operations in VGG net\n","            will be removed. Default: False.\n","        pooling_stride (int): The stride of max pooling operation. Default: 2.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 layer_name_list,\n","                 vgg_type='vgg19',\n","                 use_input_norm=True,\n","                 range_norm=False,\n","                 requires_grad=False,\n","                 remove_pooling=False,\n","                 pooling_stride=2):\n","        super(VGGFeatureExtractor, self).__init__()\n","\n","        self.layer_name_list = layer_name_list\n","        self.use_input_norm = use_input_norm\n","        self.range_norm = range_norm\n","\n","        self.names = NAMES[vgg_type.replace('_bn', '')]\n","        if 'bn' in vgg_type:\n","            self.names = insert_bn(self.names)\n","\n","        # only borrow layers that will be used to avoid unused params\n","        max_idx = 0\n","        for v in layer_name_list:\n","            idx = self.names.index(v)\n","            if idx > max_idx:\n","                max_idx = idx\n","\n","        if os.path.exists(VGG_PRETRAIN_PATH):\n","            vgg_net = getattr(vgg, vgg_type)(pretrained=False)\n","            state_dict = torch.load(VGG_PRETRAIN_PATH, map_location=lambda storage, loc: storage)\n","            vgg_net.load_state_dict(state_dict)\n","        else:\n","            vgg_net = getattr(vgg, vgg_type)(pretrained=True)\n","\n","        features = vgg_net.features[:max_idx + 1]\n","\n","        modified_net = OrderedDict()\n","        for k, v in zip(self.names, features):\n","            if 'pool' in k:\n","                # if remove_pooling is true, pooling operation will be removed\n","                if remove_pooling:\n","                    continue\n","                else:\n","                    # in some cases, we may want to change the default stride\n","                    modified_net[k] = nn.MaxPool2d(kernel_size=2, stride=pooling_stride)\n","            else:\n","                modified_net[k] = v\n","\n","        self.vgg_net = nn.Sequential(modified_net)\n","\n","        if not requires_grad:\n","            self.vgg_net.eval()\n","            for param in self.parameters():\n","                param.requires_grad = False\n","        else:\n","            self.vgg_net.train()\n","            for param in self.parameters():\n","                param.requires_grad = True\n","\n","        if self.use_input_norm:\n","            # the mean is for image with range [0, 1]\n","            self.register_buffer('mean', torch.Tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n","            # the std is for image with range [0, 1]\n","            self.register_buffer('std', torch.Tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n","\n","    def forward(self, x):\n","        \"\"\"Forward function.\n","        Args:\n","            x (Tensor): Input tensor with shape (n, c, h, w).\n","        Returns:\n","            Tensor: Forward results.\n","        \"\"\"\n","        if self.range_norm:\n","            x = (x + 1) / 2\n","        if self.use_input_norm:\n","            x = (x - self.mean) / self.std\n","\n","        output = {}\n","        for key, layer in self.vgg_net._modules.items():\n","            x = layer(x)\n","            if key in self.layer_name_list:\n","                output[key] = x.clone()\n","\n","        return output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePMmJsc3ABk6"},"outputs":[],"source":["class PerceptualLoss(nn.Module):\n","    \"\"\"Perceptual loss with commonly used style loss.\n","    Args:\n","        layer_weights (dict): The weight for each layer of vgg feature.\n","            Here is an example: {'conv5_4': 1.}, which means the conv5_4\n","            feature layer (before relu5_4) will be extracted with weight\n","            1.0 in calculating losses.\n","        vgg_type (str): The type of vgg network used as feature extractor.\n","            Default: 'vgg19'.\n","        use_input_norm (bool):  If True, normalize the input image in vgg.\n","            Default: True.\n","        range_norm (bool): If True, norm images with range [-1, 1] to [0, 1].\n","            Default: False.\n","        perceptual_weight (float): If `perceptual_weight > 0`, the perceptual\n","            loss will be calculated and the loss will multiplied by the\n","            weight. Default: 1.0.\n","        style_weight (float): If `style_weight > 0`, the style loss will be\n","            calculated and the loss will multiplied by the weight.\n","            Default: 0.\n","        criterion (str): Criterion used for perceptual loss. Default: 'l1'.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 layer_weights,\n","                 vgg_type='vgg19',\n","                 use_input_norm=True,\n","                 range_norm=False,\n","                 perceptual_weight=1.0,\n","                 style_weight=0.,\n","                 criterion='l1'):\n","        super(PerceptualLoss, self).__init__()\n","        self.perceptual_weight = perceptual_weight\n","        self.style_weight = style_weight\n","        self.layer_weights = layer_weights\n","        self.vgg = VGGFeatureExtractor(\n","            layer_name_list=list(layer_weights.keys()),\n","            vgg_type=vgg_type,\n","            use_input_norm=use_input_norm,\n","            range_norm=range_norm)\n","\n","        self.criterion_type = criterion\n","        if self.criterion_type == 'l1':\n","            self.criterion = torch.nn.L1Loss()\n","        elif self.criterion_type == 'l2':\n","            self.criterion = torch.nn.L2loss()\n","        elif self.criterion_type == 'fro':\n","            self.criterion = None\n","        else:\n","            raise NotImplementedError(f'{criterion} criterion has not been supported.')\n","\n","    def forward(self, x, gt):\n","        \"\"\"Forward function.\n","        Args:\n","            x (Tensor): Input tensor with shape (n, c, h, w).\n","            gt (Tensor): Ground-truth tensor with shape (n, c, h, w).\n","        Returns:\n","            Tensor: Forward results.\n","        \"\"\"\n","        # extract vgg features\n","        x_features = self.vgg(x)\n","        gt_features = self.vgg(gt.detach())\n","\n","        # calculate perceptual loss\n","        if self.perceptual_weight > 0:\n","            percep_loss = 0\n","            for k in x_features.keys():\n","                if self.criterion_type == 'fro':\n","                    percep_loss += torch.norm(x_features[k] - gt_features[k], p='fro') * self.layer_weights[k]\n","                else:\n","                    percep_loss += self.criterion(x_features[k], gt_features[k]) * self.layer_weights[k]\n","            percep_loss *= self.perceptual_weight\n","        else:\n","            percep_loss = None\n","\n","        # calculate style loss\n","        if self.style_weight > 0:\n","            style_loss = 0\n","            for k in x_features.keys():\n","                if self.criterion_type == 'fro':\n","                    style_loss += torch.norm(\n","                        self._gram_mat(x_features[k]) - self._gram_mat(gt_features[k]), p='fro') * self.layer_weights[k]\n","                else:\n","                    style_loss += self.criterion(self._gram_mat(x_features[k]), self._gram_mat(\n","                        gt_features[k])) * self.layer_weights[k]\n","            style_loss *= self.style_weight\n","        else:\n","            style_loss = None\n","\n","        return percep_loss\n","\n","    def _gram_mat(self, x):\n","        \"\"\"Calculate Gram matrix.\n","        Args:\n","            x (torch.Tensor): Tensor with shape of (n, c, h, w).\n","        Returns:\n","            torch.Tensor: Gram matrix.\n","        \"\"\"\n","        n, c, h, w = x.size()\n","        features = x.view(n, c, w * h)\n","        features_t = features.transpose(1, 2)\n","        gram = features.bmm(features_t) / (c * h * w)\n","        return gram\n"]},{"cell_type":"code","source":["loss_wiehgt_dic = {'conv1_2': 0.1, 'conv2_2': 0.1, 'conv3_4': 1., 'conv4_4': 1., 'conv5_4': 1.}"],"metadata":{"id":"i_T0d9Y0zRbR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Gradient Prior Loss"],"metadata":{"id":"a-KONSkBB9n2"}},{"cell_type":"code","source":["class GradientPriorLoss(nn.Module):\n","    def __init__(self, ):\n","        super(GradientPriorLoss, self).__init__()\n","        self.func = nn.L1Loss()\n","\n","    def forward(self, out_images, target_images):\n","        map_out = self.gradient_map(out_images)\n","        map_target = self.gradient_map(target_images)\n","        return self.func(map_out, map_target)\n","\n","    @staticmethod\n","    def gradient_map(x):\n","        batch_size, channel, h_x, w_x = x.size()\n","        r = F.pad(x, (0, 1, 0, 0))[:, :, :, 1:]\n","        l = F.pad(x, (1, 0, 0, 0))[:, :, :, :w_x]\n","        t = F.pad(x, (0, 0, 1, 0))[:, :, :h_x, :]\n","        b = F.pad(x, (0, 0, 0, 1))[:, :, 1:, :]\n","        xgrad = torch.pow(torch.pow((r - l) * 0.5, 2) + torch.pow((t - b) * 0.5, 2)+1e-6, 0.5)\n","        return xgrad\n","\n","class ImageLoss(nn.Module):\n","    def __init__(self, gradient=True, loss_weight=[1, 0.001]):\n","        super(ImageLoss, self).__init__()\n","        self.mse = nn.MSELoss()\n","        if gradient:\n","            self.GPLoss = GradientPriorLoss()\n","        self.gradient = gradient\n","        self.loss_weight = loss_weight\n","\n","    def forward(self, out_images, target_images):\n","        if self.gradient:\n","            loss1 = self.mse(out_images, target_images)\n","            loss2 = self.GPLoss(out_images, target_images)\n","            # print(loss1, loss2)\n","            loss = self.loss_weight[0] * loss1 + self.loss_weight[1] * loss2\n","                   \n","        else:\n","            loss = self.loss_weight[0] * self.mse(out_images, target_images)\n","        return loss"],"metadata":{"id":"Qr_aXXd2y6DE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### EdgeLoss\n","opencv canny함수 활용한 loss"],"metadata":{"id":"fSP4EaKYCH4A"}},{"cell_type":"code","source":["class EdgeLoss(nn.Module):\n","    def __init__(self, loss_weight=[1, 1]):\n","        super(EdgeLoss, self).__init__()\n","        self.loss_weight = loss_weight\n","\n","    def forward(self, out_images, target_images):\n","        o_list = []\n","        t_list = []\n","        for o, t in zip(out_images, target_images):\n","          o_list.append(torch.from_numpy(cv2.Canny((o.detach().cpu().numpy()[0].reshape(128, 128, 1)* 255).astype('uint8') , 30, 70))/255.)\n","          t_list.append(torch.from_numpy(cv2.Canny((t.detach().cpu().numpy()[0].reshape(128, 128, 1)* 255).astype('uint8') , 30, 70))/255.)\n","        loss_func = nn.L1Loss()\n","        loss = self.loss_weight[0] * loss_func(torch.stack(o_list, 0).to(CFG['DEVICE']), torch.stack(t_list, 0).to(CFG['DEVICE'])) + \\\n","                self.loss_weight[1] * loss_func(out_images, target_images)\n","        return loss"],"metadata":{"id":"eS5cIVA-Ynop"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ice2f5G83bB_"},"source":["### extract"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_XGqay6EFcUK"},"outputs":[],"source":["# !unzip -O cp949 /content/drive/MyDrive/final_project/colab/trainset/val.zip -d /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4YuGfcF6F7DT"},"outputs":[],"source":["!unzip -O cp949 /content/drive/MyDrive/final_project/colab/trainset/trainset5.zip -d /content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"13C5sbQCF70O"},"outputs":[],"source":["# !unzip -O cp949 /content/drive/MyDrive/final_project/colab/trainset/train.zip -d /content"]},{"cell_type":"markdown","metadata":{"id":"rGjezJZlWYYO"},"source":["### train model 2x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVlKjzs4NEn-"},"outputs":[],"source":["train_df = pd.read_csv('/content/train.csv')\n","val_df = pd.read_csv('/content/val.csv')\n","# test_df = pd.read_csv('/content/drive/MyDrive/final_project/colab/trainset/test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"je4ePgeQNV02","colab":{"base_uri":"https://localhost:8080/","height":424},"executionInfo":{"status":"ok","timestamp":1667887749314,"user_tz":-540,"elapsed":34,"user":{"displayName":"ᄆᄆ","userId":"04446057954788773347"}},"outputId":"622859fb-f70e-4283-da94-8c20c481bc25"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0                  lr                  hr\n","0              0  train_lr/00001.jpg  train_hr/00001.jpg\n","1              1  train_lr/00002.jpg  train_hr/00002.jpg\n","2              2  train_lr/00003.jpg  train_hr/00003.jpg\n","3              3  train_lr/00004.jpg  train_hr/00004.jpg\n","4              4  train_lr/00005.jpg  train_hr/00005.jpg\n","...          ...                 ...                 ...\n","9995        9995  train_lr/09996.jpg  train_hr/09996.jpg\n","9996        9996  train_lr/09997.jpg  train_hr/09997.jpg\n","9997        9997  train_lr/09998.jpg  train_hr/09998.jpg\n","9998        9998  train_lr/09999.jpg  train_hr/09999.jpg\n","9999        9999  train_lr/10000.jpg  train_hr/10000.jpg\n","\n","[10000 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-b92078f6-710c-4afa-9f96-aad78e32254a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>lr</th>\n","      <th>hr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>train_lr/00001.jpg</td>\n","      <td>train_hr/00001.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>train_lr/00002.jpg</td>\n","      <td>train_hr/00002.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>train_lr/00003.jpg</td>\n","      <td>train_hr/00003.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>train_lr/00004.jpg</td>\n","      <td>train_hr/00004.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>train_lr/00005.jpg</td>\n","      <td>train_hr/00005.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>9995</td>\n","      <td>train_lr/09996.jpg</td>\n","      <td>train_hr/09996.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>9996</td>\n","      <td>train_lr/09997.jpg</td>\n","      <td>train_hr/09997.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>9997</td>\n","      <td>train_lr/09998.jpg</td>\n","      <td>train_hr/09998.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>9998</td>\n","      <td>train_lr/09999.jpg</td>\n","      <td>train_hr/09999.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>9999</td>\n","      <td>train_lr/10000.jpg</td>\n","      <td>train_hr/10000.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b92078f6-710c-4afa-9f96-aad78e32254a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b92078f6-710c-4afa-9f96-aad78e32254a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b92078f6-710c-4afa-9f96-aad78e32254a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":16}],"source":["train_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"9evlB804NXiB","outputId":"e5dc85de-8f2e-467f-e970-4c451f21cea4","executionInfo":{"status":"ok","timestamp":1667887749315,"user_tz":-540,"elapsed":17,"user":{"displayName":"ᄆᄆ","userId":"04446057954788773347"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Unnamed: 0                lr                hr\n","0              0  val_lr/10001.jpg  val_hr/10001.jpg\n","1              1  val_lr/10002.jpg  val_hr/10002.jpg\n","2              2  val_lr/10003.jpg  val_hr/10003.jpg\n","3              3  val_lr/10004.jpg  val_hr/10004.jpg\n","4              4  val_lr/10005.jpg  val_hr/10005.jpg\n","...          ...               ...               ...\n","2495        2495  val_lr/12496.jpg  val_hr/12496.jpg\n","2496        2496  val_lr/12497.jpg  val_hr/12497.jpg\n","2497        2497  val_lr/12498.jpg  val_hr/12498.jpg\n","2498        2498  val_lr/12499.jpg  val_hr/12499.jpg\n","2499        2499  val_lr/12500.jpg  val_hr/12500.jpg\n","\n","[2500 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-54dfaa86-6359-4794-94d0-67c36334787f\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>lr</th>\n","      <th>hr</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>val_lr/10001.jpg</td>\n","      <td>val_hr/10001.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>val_lr/10002.jpg</td>\n","      <td>val_hr/10002.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>val_lr/10003.jpg</td>\n","      <td>val_hr/10003.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>val_lr/10004.jpg</td>\n","      <td>val_hr/10004.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>val_lr/10005.jpg</td>\n","      <td>val_hr/10005.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2495</th>\n","      <td>2495</td>\n","      <td>val_lr/12496.jpg</td>\n","      <td>val_hr/12496.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2496</th>\n","      <td>2496</td>\n","      <td>val_lr/12497.jpg</td>\n","      <td>val_hr/12497.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2497</th>\n","      <td>2497</td>\n","      <td>val_lr/12498.jpg</td>\n","      <td>val_hr/12498.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2498</th>\n","      <td>2498</td>\n","      <td>val_lr/12499.jpg</td>\n","      <td>val_hr/12499.jpg</td>\n","    </tr>\n","    <tr>\n","      <th>2499</th>\n","      <td>2499</td>\n","      <td>val_lr/12500.jpg</td>\n","      <td>val_hr/12500.jpg</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2500 rows × 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54dfaa86-6359-4794-94d0-67c36334787f')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-54dfaa86-6359-4794-94d0-67c36334787f button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-54dfaa86-6359-4794-94d0-67c36334787f');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":17}],"source":["val_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bVBcS52XNZDC"},"outputs":[],"source":["# test_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"any8QF8qWeeP","executionInfo":{"status":"ok","timestamp":1667887750058,"user_tz":-540,"elapsed":19,"user":{"displayName":"ᄆᄆ","userId":"04446057954788773347"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4b606cd2-31dd-4bda-fb4d-7447dfab099f"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2894.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["model = SwinIR(upscale=CFG['SCALE'], in_chans=1, img_size=CFG['IMG_SIZE'], window_size=4,\n","                        img_range=1., depths=[6, 6, 6, 6], embed_dim=60, num_heads=[6, 6, 6, 6],\n","                    mlp_ratio=2, upsampler='pixelshuffledirect', resi_connection='1conv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5-xHHKJWeeQ"},"outputs":[],"source":["# param_key_g = 'params'\n","# # # pretrained_dict = torch.load(r'/content/drive/MyDrive/final_project/colab/saved_models/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x2_GAN.pth')\n","# pretrained_dict = torch.load(r'/content/drive/MyDrive/final_project/colab/train_result/SwinIR_gray/002/model_state_dict19.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRaDtDawWeeR"},"outputs":[],"source":["# model.load_state_dict(pretrained_dict[param_key_g] if param_key_g in pretrained_dict.keys() else pretrained_dict, strict=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kYTkxg8xWeeR"},"outputs":[],"source":["model = model.to(CFG['DEVICE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NKaplpUAWeeS"},"outputs":[],"source":["def get_train_transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0)],\n","        additional_targets={'image': 'image', 'label': 'image'}\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrXiKTxEWeeS"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, df, transforms, train_mode):\n","        self.df = df\n","        self.transforms = transforms\n","        self.train_mode = train_mode\n","\n","    def __getitem__(self, index):\n","        lr_path = self.df['lr'].iloc[index]\n","        lr_img = np.asarray(Image.open(lr_path).convert('L'))\n","        if self.train_mode:\n","            hr_path = self.df['hr'].iloc[index]\n","            hr_img = np.asarray(Image.open(hr_path).convert('L'))\n","            if transforms is not None:\n","                transformed = self.transforms(image=lr_img, label=hr_img)\n","                lr_img = transformed['image'] / 255.\n","                hr_img = transformed['label'] / 255.\n","            return lr_img, hr_img\n","        else:\n","            file_name = lr_path.split('/')[-1]\n","            if transforms is not None:\n","                transformed = self.transforms(image=lr_img)\n","                lr_img = transformed['image'] / 255.\n","            return lr_img, file_name\n","        \n","    def __len__(self):\n","        return len(self.df)"]},{"cell_type":"code","source":[],"metadata":{"id":"mP1TjmzNCouK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### train perceptual loss"],"metadata":{"id":"8NfP4C_kCpfk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5OGxtnCWeeT"},"outputs":[],"source":["def train(model, optimizer, train_loader, scheduler, device):\n","    model.to(device)\n","    criterion = nn.L1Loss().to(device)\n","    train_losses_perceptual = []\n","    val_losses_perceptual = []\n","    train_losses_l1 = []\n","    val_losses_l1 = []\n","    criterion_perceptual = PerceptualLoss(perceptual_weight=1., style_weight=0., layer_weights=loss_wiehgt_dic).to(device)\n","    criterion_l1 = nn.L1Loss().to(device)\n","    best_model = None\n","    best_loss = 9999\n","    for epoch in range(1, CFG['EPOCHS']+1):\n","        model.train()\n","        train_loss_l1 = 0.\n","        train_loss_perceptual = 0.\n","        for lr_img, hr_img in tqdm(iter(train_loader)):\n","            lr_img, hr_img = lr_img.float().to(device), hr_img.float().to(device)\n","            \n","            optimizer.zero_grad()\n","            \n","            torch.cuda.empty_cache()\n","            pred_hr_img = model(lr_img)\n","            loss_l1 = criterion_l1(pred_hr_img, hr_img)\n","            loss_perceptual = criterion_perceptual(pred_hr_img, hr_img)\n","            loss = (loss_l1 + loss_perceptual)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss_l1 += loss_l1.item()\n","            train_loss_perceptual += loss_perceptual.item()\n","\n","        val_loss_l1 = 0.0\n","        val_loss_perceptual = 0.0\n","        val_loss = 0.0\n","        model.eval()\n","        with torch.no_grad():\n","          for lr_img, hr_img in tqdm(iter(val_loader)):\n","              lr_img, hr_img = lr_img.float().to(device), hr_img.float().to(device)\n","              predicted_outputs = model(lr_img)\n","              val_loss_l1 += criterion_l1(predicted_outputs, hr_img)\n","              val_loss_perceptual += criterion_perceptual(predicted_outputs, hr_img)\n","              val_loss += (val_loss_l1 + val_loss_perceptual).item()\n","\n","        \n","        if scheduler is not None:\n","            scheduler.step()\n","        train_losses_l1.append(float(train_loss_l1/len(train_loader)))\n","        train_losses_perceptual.append(float(train_loss_perceptual/len(train_loader)))\n","        val_losses_l1.append(float(val_loss_l1/len(val_loader)))\n","        val_losses_perceptual.append(float(val_loss_perceptual/len(val_loader)))\n","        pd.DataFrame({'train_loss_l1':train_losses_l1, 'train_loss_perceptual':train_losses_perceptual, 'val_loss_l1':val_losses_l1, 'val_loss_perceptual':val_losses_perceptual}).to_csv('/content/drive/MyDrive/final_project/colab/train_result/log.csv')\n","        print(f'Epoch : [{epoch}] Train Loss : [{train_losses_perceptual[-1]+train_losses_l1[-1]:.5f}](per:{train_losses_perceptual[-1]:.5f}, l1:{train_losses_l1[-1]:.5f}), [{val_losses_perceptual[-1]+val_losses_l1[-1]:.5f}](per:{val_losses_perceptual[-1]:.5f}, l1:{val_losses_l1[-1]:.5f})')\n","        torch.save(model.state_dict(), f\"/content/drive/MyDrive/final_project/colab/train_result/model_state_dict{epoch}.pth\")\n","        if best_loss > val_losses_l1[-1]+val_losses_perceptual[-1]:\n","            best_loss = val_loss\n","            best_model = model\n","            \n","    return best_model"]},{"cell_type":"markdown","source":["### train gradient prior loss"],"metadata":{"id":"4tiejt3jCuW4"}},{"cell_type":"code","source":["def train(model, optimizer, train_loader, scheduler, device):\n","    model.to(device)\n","    criterion = ImageLoss().to(device)\n","    best_model = None\n","    best_loss = 9999\n","    val_losses = []\n","    train_losses = []\n","    for epoch in range(1, CFG['EPOCHS']+1):\n","        model.train()\n","        train_loss = 0.\n","        for lr_img, hr_img in tqdm(iter(train_loader)):\n","            lr_img, hr_img = lr_img.float().to(device), hr_img.float().to(device)\n","            \n","            optimizer.zero_grad()\n","            \n","            torch.cuda.empty_cache()\n","            pred_hr_img = model(lr_img)\n","            loss = criterion(pred_hr_img, hr_img)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","\n","        val_loss = 0.0\n","        model.eval()\n","        with torch.no_grad():\n","          for lr_img, hr_img in tqdm(iter(val_loader)):\n","              lr_img, hr_img = lr_img.float().to(device), hr_img.float().to(device)\n","              predicted_outputs = model(lr_img)\n","              loss = criterion(predicted_outputs, hr_img)\n","              val_loss += loss.item()\n","\n","        \n","        if scheduler is not None:\n","            scheduler.step()\n","        train_losses.append(float(train_loss/len(train_loader)))\n","        val_losses.append(float(val_loss/len(val_loader)))\n","        pd.DataFrame({'train_loss':train_losses, 'val_loss':val_losses}).to_csv('/content/drive/MyDrive/final_project/colab/train_result/log.csv')\n","        print(f'Epoch : [{epoch}] Train Loss: [{train_losses[-1]:.5f}], Validation Loss: [{val_losses[-1]:.5f}]')\n","        torch.save(model.state_dict(), f\"/content/drive/MyDrive/final_project/colab/train_result/model_state_dict{epoch}.pth\")\n","        if best_loss > val_losses[-1]:\n","            best_loss = val_loss\n","            best_model = model\n","            \n","    return best_model"],"metadata":{"id":"jGxEm-VGyebP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### train canny loss"],"metadata":{"id":"041yw2rGCzxy"}},{"cell_type":"code","source":["def train(model, optimizer, train_loader, scheduler, device):\n","    model.to(device)\n","    criterion = EdgeLoss().to(device)\n","    best_model = None\n","    best_loss = 9999\n","    val_losses = []\n","    train_losses = []\n","    for epoch in range(1, CFG['EPOCHS']+1):\n","        model.train()\n","        train_loss = 0.\n","        for lr_img, hr_img in tqdm(iter(train_loader)):\n","            lr_img, hr_img = lr_img.float().to(device), hr_img.float().to(device)\n","            \n","            optimizer.zero_grad()\n","            \n","            torch.cuda.empty_cache()\n","            pred_hr_img = model(lr_img)\n","            loss = criterion(pred_hr_img, hr_img)\n","            loss.backward()\n","            optimizer.step()\n","            \n","            train_loss += loss.item()\n","\n","        val_loss = 0.0\n","        model.eval()\n","        with torch.no_grad():\n","          for lr_img, hr_img in tqdm(iter(val_loader)):\n","              lr_img, hr_img = lr_img.float().to(device), hr_img.float().to(device)\n","              predicted_outputs = model(lr_img)\n","              loss = criterion(predicted_outputs, hr_img)\n","              val_loss += loss.item()\n","\n","        \n","        if scheduler is not None:\n","            scheduler.step()\n","        train_losses.append(float(train_loss/len(train_loader)))\n","        val_losses.append(float(val_loss/len(val_loader)))\n","        pd.DataFrame({'train_loss':train_losses, 'val_loss':val_losses}).to_csv('/content/drive/MyDrive/final_project/colab/train_result/log.csv')\n","        print(f'Epoch : [{epoch}] Train Loss: [{train_losses[-1]:.5f}], Validation Loss: [{val_losses[-1]:.5f}]')\n","        torch.save(model.state_dict(), f\"/content/drive/MyDrive/final_project/colab/train_result/model_state_dict{epoch}.pth\")\n","        if best_loss > val_losses[-1]:\n","            best_loss = val_loss\n","            best_model = model\n","            \n","    return best_model"],"metadata":{"id":"CB_GK6kpYwC9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### run!"],"metadata":{"id":"srCsW-5JC4be"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFSOqyt4WeeU"},"outputs":[],"source":["train_dataset = CustomDataset(train_df, get_train_transform(), True)\n","val_dataset = CustomDataset(val_df, get_train_transform(), True)\n","train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, drop_last=True)\n","val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, drop_last=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":155,"referenced_widgets":["9184e31ca5324e198128f756b8ab5374","916b5566f987449fa0695638c47fce8e","ef5f386085e347bebce5499887ad7383","b02a2c091f9340899ce636ac8e785719","fc663090d6b645039d744782ca16b98b","bc228504a14d4e3a9de76d4e439b454a","8f40e0dfbce9490eb5677b5d741e8227","17d65915dcb74338a74020b09f89d95a","00650a94aefb4a788884c378cf31f8e8","f3dbb25379ec4fbf8bc80980e43454da","e1b18c46985143fb857fb1be3bbfa276","a5b96d0c48954c379639bbb295779e79","e802c5e6ba074251a8d8d64144b66ae3","9ff0e1ff1fc54a92bb63c55ca11c679e","fec4f132d1ca4ced844d2124d5513ce7","c06b3d1edf4647ae878daa466eb8ffad","c511a95af7414c208cd8729dceedc9aa","268501a219bf49b880a0d398b17f6926","6a394b05808d4ca5853c376de4850207","26decb304c644199a605581104d76eab","a6bd8df6643b4507a0e18f67579c36e4","8cf49f358eed40f1aebf55fd00397ee8"]},"id":"UnH2mLBpWeeV","outputId":"b7fe4ba5-2aaf-4f32-94c8-922aad2500b0","executionInfo":{"status":"ok","timestamp":1667887797275,"user_tz":-540,"elapsed":43621,"user":{"displayName":"ᄆᄆ","userId":"04446057954788773347"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/15 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9184e31ca5324e198128f756b8ab5374"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/albumentations/pytorch/transforms.py:88: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:172.)\n","  return torch.from_numpy(img.transpose(2, 0, 1))\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5b96d0c48954c379639bbb295779e79"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch : [1] Train Loss: [0.14740], Validation Loss: [0.04873]\n"]}],"source":["optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n","scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n","                                        lr_lambda=lambda epoch: 0.95 ** epoch)\n","\n","infer_model = train(model, optimizer, train_loader, None, CFG['DEVICE'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PlQAchTO3gbB"},"outputs":[],"source":["torch.save(infer_model.state_dict(), f\"/content/drive/MyDrive/final_project/colab/train_result/infer_model.pth\")"]},{"cell_type":"markdown","metadata":{"id":"arVx6CuTkXhG"},"source":["### train lightwiehgt model by tpu(Failed)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBejxlHLkikb","outputId":"e33c94e5-3674-4b4c-dedf-e0114186b85b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torch-xla==1.12\n","  Downloading https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl (187.4 MB)\n","\u001b[K     |████████████████████████████████| 187.4 MB 29 kB/s \n","\u001b[?25hCollecting cloud-tpu-client==0.10\n","  Downloading cloud_tpu_client-0.10-py3-none-any.whl (7.4 kB)\n","Collecting torch==1.12.0\n","  Downloading torch-1.12.0-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[K     |████████████████████████████████| 776.3 MB 14 kB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from torch-xla==1.12) (1.2.0)\n","Collecting google-api-python-client==1.8.0\n","  Downloading google_api_python_client-1.8.0-py3-none-any.whl (57 kB)\n","\u001b[K     |████████████████████████████████| 57 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: oauth2client in /usr/local/lib/python3.7/dist-packages (from cloud-tpu-client==0.10) (4.1.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.0) (4.1.1)\n","Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.1)\n","Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.0.4)\n","Requirement already satisfied: google-api-core<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.31.6)\n","Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.35.0)\n","Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.17.4)\n","Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.15.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.56.4)\n","Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (21.3)\n","Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.23.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2022.4)\n","Requirement already satisfied: protobuf<4.0.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.17.3)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (57.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (4.9)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.9)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.4.1->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (0.4.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2dev,>=1.13.0->google-api-python-client==1.8.0->cloud-tpu-client==0.10) (2022.9.24)\n","Installing collected packages: google-api-python-client, cloud-tpu-client, torch-xla, torch\n","  Attempting uninstall: google-api-python-client\n","    Found existing installation: google-api-python-client 1.12.11\n","    Uninstalling google-api-python-client-1.12.11:\n","      Successfully uninstalled google-api-python-client-1.12.11\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.1+cu113\n","    Uninstalling torch-1.12.1+cu113:\n","      Successfully uninstalled torch-1.12.1+cu113\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","torchvision 0.13.1+cu113 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n","torchtext 0.13.1 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n","torchaudio 0.12.1+cu113 requires torch==1.12.1, but you have torch 1.12.0 which is incompatible.\n","earthengine-api 0.1.326 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 1.8.0 which is incompatible.\u001b[0m\n","Successfully installed cloud-tpu-client-0.10 google-api-python-client-1.8.0 torch-1.12.0 torch-xla-1.12\n"]}],"source":["# Installs PyTorch, PyTorch/XLA, and Torchvision\n","# Copy this cell into your own notebooks to use PyTorch on Cloud TPUs \n","# Warning: this may take a couple minutes to run\n","!pip install cloud-tpu-client==0.10 torch==1.12.0 https://storage.googleapis.com/tpu-pytorch/wheels/colab/torch_xla-1.12-cp37-cp37m-linux_x86_64.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"INiCE0Db4Ay4","outputId":"f5e1af3b-b9b3-4922-c777-16015326701e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.11-py3-none-any.whl (548 kB)\n","\u001b[K     |████████████████████████████████| 548 kB 5.0 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.7/dist-packages (from timm) (1.12.0)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.13.1+cu113)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 52.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from timm) (6.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7->timm) (4.1.1)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (5.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (3.8.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub->timm) (3.9.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub->timm) (2022.9.24)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.6)\n","Collecting torch>=1.7\n","  Downloading torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl (776.3 MB)\n","\u001b[K     |████████████████████████████████| 776.3 MB 10 kB/s \n","\u001b[?25hRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n","Installing collected packages: torch, huggingface-hub, timm\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.12.0\n","    Uninstalling torch-1.12.0:\n","      Successfully uninstalled torch-1.12.0\n","Successfully installed huggingface-hub-0.10.1 timm-0.6.11 torch-1.12.1\n"]}],"source":["!pip install timm # Installing timm, a computer vision library for pytorch\n","# Downloading a swin IR Model.\n","# More models can be found here: https://github.com/JingyunLiang/SwinIR/releases/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkL39El3kb-k"},"outputs":[],"source":["import os\n","assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GcbeJk7jl1Up","outputId":"7b097627-3aa3-4f72-af4c-569440216904"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n","  warn(f\"Failed to load image Python extension: {e}\")\n"]}],"source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torch_xla\n","import torch_xla.core.xla_model as xm\n","import torch_xla.distributed.xla_multiprocessing as xmp\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","import torchvision\n","import torchvision.models as models\n","from torchvision import transforms\n","import matplotlib.pyplot as plt\n","import torch_xla.distributed.parallel_loader as pl\n","import time\n","import random\n","from PIL import Image\n","import numpy as np\n","import pickle\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77eO8Wycfkb4","outputId":"ef4fc9af-16e0-4efe-ee87-189b03e4bcae"},"outputs":[{"data":{"text/plain":["['xla:1', 'xla:2', 'xla:3', 'xla:4', 'xla:5', 'xla:6', 'xla:7', 'xla:8']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["xm.get_xla_supported_devices()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S4HvmF6j4Ay7","outputId":"a9c9a1ca-a5e4-46e1-b5f4-b6454e220346"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TUcz5q6TGQ9r"},"outputs":[],"source":["with open('/content/drive/MyDrive/final_project/colab/data/train_df.pkl', 'rb') as f:\n","    train_df = pickle.load(f)\n","with open('/content/drive/MyDrive/final_project/colab/data/val_df.pkl', 'rb') as f:\n","    val_df = pickle.load(f)\n","# train_df = pd.read_csv('/content/drive/MyDrive/final_project/colab/data/train_df_test.csv')\n","# val_df = pd.read_csv('/content/drive/MyDrive/final_project/colab/data/val_df_test.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"82OtNOjb4Ay7"},"outputs":[],"source":["# We need the code for the SwinIR Model. Copying that file will give us a Pytorch model called SwinIR\n","# We can load the weights that we downloaded above into the SwinIR Object\n","\n","import math\n","import torch.utils.checkpoint as checkpoint\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n","        super().__init__()\n","        out_features = out_features or in_features\n","        hidden_features = hidden_features or in_features\n","        self.fc1 = nn.Linear(in_features, hidden_features)\n","        self.act = act_layer()\n","        self.fc2 = nn.Linear(hidden_features, out_features)\n","        self.drop = nn.Dropout(drop)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act(x)\n","        x = self.drop(x)\n","        x = self.fc2(x)\n","        x = self.drop(x)\n","        return x\n","\n","\n","def window_partition(x, window_size):\n","    \"\"\"\n","    Args:\n","        x: (B, H, W, C)\n","        window_size (int): window size\n","\n","    Returns:\n","        windows: (num_windows*B, window_size, window_size, C)\n","    \"\"\"\n","    B, H, W, C = x.shape\n","    x = x.view(B, H // window_size, window_size, W // window_size, window_size, C)\n","    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n","    return windows\n","\n","\n","def window_reverse(windows, window_size, H, W):\n","    \"\"\"\n","    Args:\n","        windows: (num_windows*B, window_size, window_size, C)\n","        window_size (int): Window size\n","        H (int): Height of image\n","        W (int): Width of image\n","\n","    Returns:\n","        x: (B, H, W, C)\n","    \"\"\"\n","    B = int(windows.shape[0] / (H * W / window_size / window_size))\n","    x = windows.view(B, H // window_size, W // window_size, window_size, window_size, -1)\n","    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, -1)\n","    return x\n","\n","\n","class WindowAttention(nn.Module):\n","    r\"\"\" Window based multi-head self attention (W-MSA) module with relative position bias.\n","    It supports both of shifted and non-shifted window.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        window_size (tuple[int]): The height and width of the window.\n","        num_heads (int): Number of attention heads.\n","        qkv_bias (bool, optional):  If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set\n","        attn_drop (float, optional): Dropout ratio of attention weight. Default: 0.0\n","        proj_drop (float, optional): Dropout ratio of output. Default: 0.0\n","    \"\"\"\n","\n","    def __init__(self, dim, window_size, num_heads, qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.window_size = window_size  # Wh, Ww\n","        self.num_heads = num_heads\n","        head_dim = dim // num_heads\n","        self.scale = qk_scale or head_dim ** -0.5\n","\n","        # define a parameter table of relative position bias\n","        self.relative_position_bias_table = nn.Parameter(\n","            torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))  # 2*Wh-1 * 2*Ww-1, nH\n","\n","        # get pair-wise relative position index for each token inside the window\n","        coords_h = torch.arange(self.window_size[0])\n","        coords_w = torch.arange(self.window_size[1])\n","        coords = torch.stack(torch.meshgrid([coords_h, coords_w]))  # 2, Wh, Ww\n","        coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n","        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n","        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n","        relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n","        relative_coords[:, :, 1] += self.window_size[1] - 1\n","        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n","        relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n","        self.register_buffer(\"relative_position_index\", relative_position_index)\n","\n","        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attn_drop)\n","        self.proj = nn.Linear(dim, dim)\n","\n","        self.proj_drop = nn.Dropout(proj_drop)\n","\n","        trunc_normal_(self.relative_position_bias_table, std=.02)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x, mask=None):\n","        \"\"\"\n","        Args:\n","            x: input features with shape of (num_windows*B, N, C)\n","            mask: (0/-inf) mask with shape of (num_windows, Wh*Ww, Wh*Ww) or None\n","        \"\"\"\n","        B_, N, C = x.shape\n","        qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n","        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n","\n","        q = q * self.scale\n","        attn = (q @ k.transpose(-2, -1))\n","\n","        relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n","            self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n","        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n","        attn = attn + relative_position_bias.unsqueeze(0)\n","\n","        if mask is not None:\n","            nW = mask.shape[0]\n","            attn = attn.view(B_ // nW, nW, self.num_heads, N, N) + mask.unsqueeze(1).unsqueeze(0)\n","            attn = attn.view(-1, self.num_heads, N, N)\n","            attn = self.softmax(attn)\n","        else:\n","            attn = self.softmax(attn)\n","\n","        attn = self.attn_drop(attn)\n","\n","        x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n","        x = self.proj(x)\n","        x = self.proj_drop(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f'dim={self.dim}, window_size={self.window_size}, num_heads={self.num_heads}'\n","\n","    def flops(self, N):\n","        # calculate flops for 1 window with token length of N\n","        flops = 0\n","        # qkv = self.qkv(x)\n","        flops += N * self.dim * 3 * self.dim\n","        # attn = (q @ k.transpose(-2, -1))\n","        flops += self.num_heads * N * (self.dim // self.num_heads) * N\n","        #  x = (attn @ v)\n","        flops += self.num_heads * N * N * (self.dim // self.num_heads)\n","        # x = self.proj(x)\n","        flops += N * self.dim * self.dim\n","        return flops\n","\n","\n","class SwinTransformerBlock(nn.Module):\n","    r\"\"\" Swin Transformer Block.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resulotion.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Window size.\n","        shift_size (int): Shift size for SW-MSA.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float, optional): Stochastic depth rate. Default: 0.0\n","        act_layer (nn.Module, optional): Activation layer. Default: nn.GELU\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, num_heads, window_size=7, shift_size=0,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0., drop_path=0.,\n","                 act_layer=nn.GELU, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.num_heads = num_heads\n","        self.window_size = window_size\n","        self.shift_size = shift_size\n","        self.mlp_ratio = mlp_ratio\n","        if min(self.input_resolution) <= self.window_size:\n","            # if window size is larger than input resolution, we don't partition windows\n","            self.shift_size = 0\n","            self.window_size = min(self.input_resolution)\n","        assert 0 <= self.shift_size < self.window_size, \"shift_size must in 0-window_size\"\n","\n","        self.norm1 = norm_layer(dim)\n","        self.attn = WindowAttention(\n","            dim, window_size=to_2tuple(self.window_size), num_heads=num_heads,\n","            qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n","\n","        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n","        self.norm2 = norm_layer(dim)\n","        mlp_hidden_dim = int(dim * mlp_ratio)\n","        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n","\n","        if self.shift_size > 0:\n","            attn_mask = self.calculate_mask(self.input_resolution)\n","        else:\n","            attn_mask = None\n","\n","        self.register_buffer(\"attn_mask\", attn_mask)\n","\n","    def calculate_mask(self, x_size):\n","        # calculate attention mask for SW-MSA\n","        H, W = x_size\n","        img_mask = torch.zeros((1, H, W, 1))  # 1 H W 1\n","        h_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        w_slices = (slice(0, -self.window_size),\n","                    slice(-self.window_size, -self.shift_size),\n","                    slice(-self.shift_size, None))\n","        cnt = 0\n","        for h in h_slices:\n","            for w in w_slices:\n","                img_mask[:, h, w, :] = cnt\n","                cnt += 1\n","\n","        mask_windows = window_partition(img_mask, self.window_size)  # nW, window_size, window_size, 1\n","        mask_windows = mask_windows.view(-1, self.window_size * self.window_size)\n","        attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n","        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n","\n","        return attn_mask\n","\n","    def forward(self, x, x_size):\n","        H, W = x_size\n","        B, L, C = x.shape\n","        # assert L == H * W, \"input feature has wrong size\"\n","\n","        shortcut = x\n","        x = self.norm1(x)\n","        x = x.view(B, H, W, C)\n","\n","        # cyclic shift\n","        if self.shift_size > 0:\n","            shifted_x = torch.roll(x, shifts=(-self.shift_size, -self.shift_size), dims=(1, 2))\n","        else:\n","            shifted_x = x\n","\n","        # partition windows\n","        x_windows = window_partition(shifted_x, self.window_size)  # nW*B, window_size, window_size, C\n","        x_windows = x_windows.view(-1, self.window_size * self.window_size, C)  # nW*B, window_size*window_size, C\n","\n","        # W-MSA/SW-MSA (to be compatible for testing on images whose shapes are the multiple of window size\n","        if self.input_resolution == x_size:\n","            attn_windows = self.attn(x_windows, mask=self.attn_mask)  # nW*B, window_size*window_size, C\n","        else:\n","            attn_windows = self.attn(x_windows, mask=self.calculate_mask(x_size).to(x.device))\n","\n","        # merge windows\n","        attn_windows = attn_windows.view(-1, self.window_size, self.window_size, C)\n","        shifted_x = window_reverse(attn_windows, self.window_size, H, W)  # B H' W' C\n","\n","        # reverse cyclic shift\n","        if self.shift_size > 0:\n","            x = torch.roll(shifted_x, shifts=(self.shift_size, self.shift_size), dims=(1, 2))\n","        else:\n","            x = shifted_x\n","        x = x.view(B, H * W, C)\n","\n","        # FFN\n","        x = shortcut + self.drop_path(x)\n","        x = x + self.drop_path(self.mlp(self.norm2(x)))\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, num_heads={self.num_heads}, \" \\\n","               f\"window_size={self.window_size}, shift_size={self.shift_size}, mlp_ratio={self.mlp_ratio}\"\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.input_resolution\n","        # norm1\n","        flops += self.dim * H * W\n","        # W-MSA/SW-MSA\n","        nW = H * W / self.window_size / self.window_size\n","        flops += nW * self.attn.flops(self.window_size * self.window_size)\n","        # mlp\n","        flops += 2 * H * W * self.dim * self.dim * self.mlp_ratio\n","        # norm2\n","        flops += self.dim * H * W\n","        return flops\n","\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"input_resolution={self.input_resolution}, dim={self.dim}\"\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.dim\n","        flops += (H // 2) * (W // 2) * 4 * self.dim * 2 * self.dim\n","        return flops\n","\n","\n","class BasicLayer(nn.Module):\n","    \"\"\" A basic Swin Transformer layer for one stage.\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False):\n","\n","        super().__init__()\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","        self.depth = depth\n","        self.use_checkpoint = use_checkpoint\n","\n","        # build blocks\n","        self.blocks = nn.ModuleList([\n","            SwinTransformerBlock(dim=dim, input_resolution=input_resolution,\n","                                 num_heads=num_heads, window_size=window_size,\n","                                 shift_size=0 if (i % 2 == 0) else window_size // 2,\n","                                 mlp_ratio=mlp_ratio,\n","                                 qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                 drop=drop, attn_drop=attn_drop,\n","                                 drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n","                                 norm_layer=norm_layer)\n","            for i in range(depth)])\n","\n","        # patch merging layer\n","        if downsample is not None:\n","            self.downsample = downsample(input_resolution, dim=dim, norm_layer=norm_layer)\n","        else:\n","            self.downsample = None\n","\n","    def forward(self, x, x_size):\n","        for blk in self.blocks:\n","            if self.use_checkpoint:\n","                x = checkpoint.checkpoint(blk, x, x_size)\n","            else:\n","                x = blk(x, x_size)\n","        if self.downsample is not None:\n","            x = self.downsample(x)\n","        return x\n","\n","    def extra_repr(self) -> str:\n","        return f\"dim={self.dim}, input_resolution={self.input_resolution}, depth={self.depth}\"\n","\n","    def flops(self):\n","        flops = 0\n","        for blk in self.blocks:\n","            flops += blk.flops()\n","        if self.downsample is not None:\n","            flops += self.downsample.flops()\n","        return flops\n","\n","\n","class RSTB(nn.Module):\n","    \"\"\"Residual Swin Transformer Block (RSTB).\n","\n","    Args:\n","        dim (int): Number of input channels.\n","        input_resolution (tuple[int]): Input resolution.\n","        depth (int): Number of blocks.\n","        num_heads (int): Number of attention heads.\n","        window_size (int): Local window size.\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim.\n","        qkv_bias (bool, optional): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float | None, optional): Override default qk scale of head_dim ** -0.5 if set.\n","        drop (float, optional): Dropout rate. Default: 0.0\n","        attn_drop (float, optional): Attention dropout rate. Default: 0.0\n","        drop_path (float | tuple[float], optional): Stochastic depth rate. Default: 0.0\n","        norm_layer (nn.Module, optional): Normalization layer. Default: nn.LayerNorm\n","        downsample (nn.Module | None, optional): Downsample layer at the end of the layer. Default: None\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False.\n","        img_size: Input image size.\n","        patch_size: Patch size.\n","        resi_connection: The convolutional block before residual connection.\n","    \"\"\"\n","\n","    def __init__(self, dim, input_resolution, depth, num_heads, window_size,\n","                 mlp_ratio=4., qkv_bias=True, qk_scale=None, drop=0., attn_drop=0.,\n","                 drop_path=0., norm_layer=nn.LayerNorm, downsample=None, use_checkpoint=False,\n","                 img_size=224, patch_size=4, resi_connection='1conv'):\n","        super(RSTB, self).__init__()\n","\n","        self.dim = dim\n","        self.input_resolution = input_resolution\n","\n","        self.residual_group = BasicLayer(dim=dim,\n","                                         input_resolution=input_resolution,\n","                                         depth=depth,\n","                                         num_heads=num_heads,\n","                                         window_size=window_size,\n","                                         mlp_ratio=mlp_ratio,\n","                                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                                         drop=drop, attn_drop=attn_drop,\n","                                         drop_path=drop_path,\n","                                         norm_layer=norm_layer,\n","                                         downsample=downsample,\n","                                         use_checkpoint=use_checkpoint)\n","\n","        if resi_connection == '1conv':\n","            self.conv = nn.Conv2d(dim, dim, 3, 1, 1)\n","        elif resi_connection == '3conv':\n","            # to save parameters and memory\n","            self.conv = nn.Sequential(nn.Conv2d(dim, dim // 4, 3, 1, 1), nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                      nn.Conv2d(dim // 4, dim // 4, 1, 1, 0),\n","                                      nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                      nn.Conv2d(dim // 4, dim, 3, 1, 1))\n","\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n","            norm_layer=None)\n","\n","        self.patch_unembed = PatchUnEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=0, embed_dim=dim,\n","            norm_layer=None)\n","\n","    def forward(self, x, x_size):\n","        return self.patch_embed(self.conv(self.patch_unembed(self.residual_group(x, x_size), x_size))) + x\n","\n","    def flops(self):\n","        flops = 0\n","        flops += self.residual_group.flops()\n","        H, W = self.input_resolution\n","        flops += H * W * self.dim * self.dim * 9\n","        flops += self.patch_embed.flops()\n","        flops += self.patch_unembed.flops()\n","\n","        return flops\n","\n","\n","class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        x = x.flatten(2).transpose(1, 2)  # B Ph*Pw C\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.img_size\n","        if self.norm is not None:\n","            flops += H * W * self.embed_dim\n","        return flops\n","\n","\n","class PatchUnEmbed(nn.Module):\n","    r\"\"\" Image to Patch Unembedding\n","\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","    def forward(self, x, x_size):\n","        B, HW, C = x.shape\n","        x = x.transpose(1, 2).view(B, self.embed_dim, x_size[0], x_size[1])  # B Ph*Pw C\n","        return x\n","\n","    def flops(self):\n","        flops = 0\n","        return flops\n","\n","\n","class Upsample(nn.Sequential):\n","    \"\"\"Upsample module.\n","\n","    Args:\n","        scale (int): Scale factor. Supported scales: 2^n and 3.\n","        num_feat (int): Channel number of intermediate features.\n","    \"\"\"\n","\n","    def __init__(self, scale, num_feat):\n","        m = []\n","        if (scale & (scale - 1)) == 0:  # scale = 2^n\n","            for _ in range(int(math.log(scale, 2))):\n","                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n","                m.append(nn.PixelShuffle(2))\n","        elif scale == 3:\n","            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n","            m.append(nn.PixelShuffle(3))\n","        else:\n","            raise ValueError(f'scale {scale} is not supported. ' 'Supported scales: 2^n and 3.')\n","        super(Upsample, self).__init__(*m)\n","\n","\n","class UpsampleOneStep(nn.Sequential):\n","    \"\"\"UpsampleOneStep module (the difference with Upsample is that it always only has 1conv + 1pixelshuffle)\n","       Used in lightweight SR to save parameters.\n","\n","    Args:\n","        scale (int): Scale factor. Supported scales: 2^n and 3.\n","        num_feat (int): Channel number of intermediate features.\n","\n","    \"\"\"\n","\n","    def __init__(self, scale, num_feat, num_out_ch, input_resolution=None):\n","        self.num_feat = num_feat\n","        self.input_resolution = input_resolution\n","        m = []\n","        m.append(nn.Conv2d(num_feat, (scale ** 2) * num_out_ch, 3, 1, 1))\n","        m.append(nn.PixelShuffle(scale))\n","        super(UpsampleOneStep, self).__init__(*m)\n","\n","    def flops(self):\n","        H, W = self.input_resolution\n","        flops = H * W * self.num_feat * 3 * 9\n","        return flops\n","\n","\n","class SwinIR(nn.Module):\n","    r\"\"\" SwinIR\n","        A PyTorch impl of : `SwinIR: Image Restoration Using Swin Transformer`, based on Swin Transformer.\n","\n","    Args:\n","        img_size (int | tuple(int)): Input image size. Default 64\n","        patch_size (int | tuple(int)): Patch size. Default: 1\n","        in_chans (int): Number of input image channels. Default: 3\n","        embed_dim (int): Patch embedding dimension. Default: 96\n","        depths (tuple(int)): Depth of each Swin Transformer layer.\n","        num_heads (tuple(int)): Number of attention heads in different layers.\n","        window_size (int): Window size. Default: 7\n","        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4\n","        qkv_bias (bool): If True, add a learnable bias to query, key, value. Default: True\n","        qk_scale (float): Override default qk scale of head_dim ** -0.5 if set. Default: None\n","        drop_rate (float): Dropout rate. Default: 0\n","        attn_drop_rate (float): Attention dropout rate. Default: 0\n","        drop_path_rate (float): Stochastic depth rate. Default: 0.1\n","        norm_layer (nn.Module): Normalization layer. Default: nn.LayerNorm.\n","        ape (bool): If True, add absolute position embedding to the patch embedding. Default: False\n","        patch_norm (bool): If True, add normalization after patch embedding. Default: True\n","        use_checkpoint (bool): Whether to use checkpointing to save memory. Default: False\n","        upscale: Upscale factor. 2/3/4/8 for image SR, 1 for denoising and compress artifact reduction\n","        img_range: Image range. 1. or 255.\n","        upsampler: The reconstruction reconstruction module. 'pixelshuffle'/'pixelshuffledirect'/'nearest+conv'/None\n","        resi_connection: The convolutional block before residual connection. '1conv'/'3conv'\n","    \"\"\"\n","\n","    def __init__(self, img_size=64, patch_size=1, in_chans=3,\n","                 embed_dim=96, depths=[6, 6, 6, 6], num_heads=[6, 6, 6, 6],\n","                 window_size=7, mlp_ratio=4., qkv_bias=True, qk_scale=None,\n","                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0.1,\n","                 norm_layer=nn.LayerNorm, ape=False, patch_norm=True,\n","                 use_checkpoint=False, upscale=2, img_range=1., upsampler='', resi_connection='1conv',\n","                 **kwargs):\n","        super(SwinIR, self).__init__()\n","        num_in_ch = in_chans\n","        num_out_ch = in_chans\n","        num_feat = 64\n","        self.img_range = img_range\n","        if in_chans == 3:\n","            rgb_mean = (0.4488, 0.4371, 0.4040)\n","            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n","        else:\n","            self.mean = torch.zeros(1, 1, 1, 1)\n","        self.upscale = upscale\n","        self.upsampler = upsampler\n","        self.window_size = window_size\n","\n","        #####################################################################################################\n","        ################################### 1, shallow feature extraction ###################################\n","        self.conv_first = nn.Conv2d(num_in_ch, embed_dim, 3, 1, 1)\n","\n","        #####################################################################################################\n","        ################################### 2, deep feature extraction ######################################\n","        self.num_layers = len(depths)\n","        self.embed_dim = embed_dim\n","        self.ape = ape\n","        self.patch_norm = patch_norm\n","        self.num_features = embed_dim\n","        self.mlp_ratio = mlp_ratio\n","\n","        # split image into non-overlapping patches\n","        self.patch_embed = PatchEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","        num_patches = self.patch_embed.num_patches\n","        patches_resolution = self.patch_embed.patches_resolution\n","        self.patches_resolution = patches_resolution\n","\n","        # merge non-overlapping patches into image\n","        self.patch_unembed = PatchUnEmbed(\n","            img_size=img_size, patch_size=patch_size, in_chans=embed_dim, embed_dim=embed_dim,\n","            norm_layer=norm_layer if self.patch_norm else None)\n","\n","        # absolute position embedding\n","        if self.ape:\n","            self.absolute_pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n","            trunc_normal_(self.absolute_pos_embed, std=.02)\n","\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","\n","        # stochastic depth\n","        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]  # stochastic depth decay rule\n","\n","        # build Residual Swin Transformer blocks (RSTB)\n","        self.layers = nn.ModuleList()\n","        for i_layer in range(self.num_layers):\n","            layer = RSTB(dim=embed_dim,\n","                         input_resolution=(patches_resolution[0],\n","                                           patches_resolution[1]),\n","                         depth=depths[i_layer],\n","                         num_heads=num_heads[i_layer],\n","                         window_size=window_size,\n","                         mlp_ratio=self.mlp_ratio,\n","                         qkv_bias=qkv_bias, qk_scale=qk_scale,\n","                         drop=drop_rate, attn_drop=attn_drop_rate,\n","                         drop_path=dpr[sum(depths[:i_layer]):sum(depths[:i_layer + 1])],  # no impact on SR results\n","                         norm_layer=norm_layer,\n","                         downsample=None,\n","                         use_checkpoint=use_checkpoint,\n","                         img_size=img_size,\n","                         patch_size=patch_size,\n","                         resi_connection=resi_connection\n","\n","                         )\n","            self.layers.append(layer)\n","        self.norm = norm_layer(self.num_features)\n","\n","        # build the last conv layer in deep feature extraction\n","        if resi_connection == '1conv':\n","            self.conv_after_body = nn.Conv2d(embed_dim, embed_dim, 3, 1, 1)\n","        elif resi_connection == '3conv':\n","            # to save parameters and memory\n","            self.conv_after_body = nn.Sequential(nn.Conv2d(embed_dim, embed_dim // 4, 3, 1, 1),\n","                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                                 nn.Conv2d(embed_dim // 4, embed_dim // 4, 1, 1, 0),\n","                                                 nn.LeakyReLU(negative_slope=0.2, inplace=True),\n","                                                 nn.Conv2d(embed_dim // 4, embed_dim, 3, 1, 1))\n","\n","        #####################################################################################################\n","        ################################ 3, high quality image reconstruction ################################\n","        if self.upsampler == 'pixelshuffle':\n","            # for classical SR\n","            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n","                                                      nn.LeakyReLU(inplace=True))\n","            self.upsample = Upsample(upscale, num_feat)\n","            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n","        elif self.upsampler == 'pixelshuffledirect':\n","            # for lightweight SR (to save parameters)\n","            self.upsample = UpsampleOneStep(upscale, embed_dim, num_out_ch,\n","                                            (patches_resolution[0], patches_resolution[1]))\n","        elif self.upsampler == 'nearest+conv':\n","            # for real-world SR (less artifacts)\n","            self.conv_before_upsample = nn.Sequential(nn.Conv2d(embed_dim, num_feat, 3, 1, 1),\n","                                                      nn.LeakyReLU(inplace=True))\n","            self.conv_up1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            if self.upscale == 4:\n","                self.conv_up2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            self.conv_hr = nn.Conv2d(num_feat, num_feat, 3, 1, 1)\n","            self.conv_last = nn.Conv2d(num_feat, num_out_ch, 3, 1, 1)\n","            self.lrelu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n","        else:\n","            # for image denoising and JPEG compression artifact reduction\n","            self.conv_last = nn.Conv2d(embed_dim, num_out_ch, 3, 1, 1)\n","\n","        self.apply(self._init_weights)\n","\n","    def _init_weights(self, m):\n","        if isinstance(m, nn.Linear):\n","            trunc_normal_(m.weight, std=.02)\n","            if isinstance(m, nn.Linear) and m.bias is not None:\n","                nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.LayerNorm):\n","            nn.init.constant_(m.bias, 0)\n","            nn.init.constant_(m.weight, 1.0)\n","\n","    @torch.jit.ignore\n","    def no_weight_decay(self):\n","        return {'absolute_pos_embed'}\n","\n","    @torch.jit.ignore\n","    def no_weight_decay_keywords(self):\n","        return {'relative_position_bias_table'}\n","\n","    def check_image_size(self, x):\n","        _, _, h, w = x.size()\n","        mod_pad_h = (self.window_size - h % self.window_size) % self.window_size\n","        mod_pad_w = (self.window_size - w % self.window_size) % self.window_size\n","        x = F.pad(x, (0, mod_pad_w, 0, mod_pad_h), 'reflect')\n","        return x\n","\n","    def forward_features(self, x):\n","        x_size = (x.shape[2], x.shape[3])\n","        x = self.patch_embed(x)\n","        if self.ape:\n","            x = x + self.absolute_pos_embed\n","        x = self.pos_drop(x)\n","\n","        for layer in self.layers:\n","            x = layer(x, x_size)\n","\n","        x = self.norm(x)  # B L C\n","        x = self.patch_unembed(x, x_size)\n","\n","        return x\n","\n","    def forward(self, x):\n","        H, W = x.shape[2:]\n","        x = self.check_image_size(x)\n","        \n","        self.mean = self.mean.type_as(x)\n","        x = (x - self.mean) * self.img_range\n","\n","        if self.upsampler == 'pixelshuffle':\n","            # for classical SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.conv_before_upsample(x)\n","            x = self.conv_last(self.upsample(x))\n","        elif self.upsampler == 'pixelshuffledirect':\n","            # for lightweight SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.upsample(x)\n","        elif self.upsampler == 'nearest+conv':\n","            # for real-world SR\n","            x = self.conv_first(x)\n","            x = self.conv_after_body(self.forward_features(x)) + x\n","            x = self.conv_before_upsample(x)\n","            x = self.lrelu(self.conv_up1(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n","            if self.upscale == 4:\n","                x = self.lrelu(self.conv_up2(torch.nn.functional.interpolate(x, scale_factor=2, mode='nearest')))\n","            x = self.conv_last(self.lrelu(self.conv_hr(x)))\n","        else:\n","            # for image denoising and JPEG compression artifact reduction\n","            x_first = self.conv_first(x)\n","            res = self.conv_after_body(self.forward_features(x_first)) + x_first\n","            x = x + self.conv_last(res)\n","\n","        x = x / self.img_range + self.mean\n","\n","        return x[:, :, :H*self.upscale, :W*self.upscale]\n","\n","    def flops(self):\n","        flops = 0\n","        H, W = self.patches_resolution\n","        flops += H * W * 3 * self.embed_dim * 9\n","        flops += self.patch_embed.flops()\n","        for i, layer in enumerate(self.layers):\n","            flops += layer.flops()\n","        flops += H * W * 3 * self.embed_dim * self.embed_dim\n","        flops += self.upsample.flops()\n","        return flops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6WCllYXfkXhI"},"outputs":[],"source":["flags = {\n","    'img_size':(320, 224),\n","    'epochs':2,\n","    'learning_rate':1e-3,\n","    'batch_size':32,\n","    'seed':0,\n","    'num_workers':8\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L-a6nLarZN5O"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True\n","\n","seed_everything(flags['seed'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bxVSnMgakXhM"},"outputs":[],"source":["class CustomDataset(Dataset):\n","    def __init__(self, df, transforms, train_mode):\n","        self.df = df\n","        self.transforms = transforms\n","        self.train_mode = train_mode\n","\n","    def __getitem__(self, index):\n","        lr_path = self.df['lr'].iloc[index]\n","        lr_img = np.asarray(Image.open(lr_path).convert('L'))[:, :, np.newaxis] #cv2.imread(lr_path, cv2.IMREAD_GRAYSCALE)\n","        if self.train_mode:\n","            hr_path = self.df['hr'].iloc[index]\n","            hr_img = np.asarray(Image.open(hr_path).convert('L'))[:, :, np.newaxis]#cv2.imread(hr_path, cv2.IMREAD_GRAYSCALE)\n","            if transforms is not None:\n","                transformed = self.transforms(image=lr_img, label=hr_img)\n","                lr_img = transformed['image'] / 255.\n","                hr_img = transformed['label'] / 255.\n","            return lr_img, hr_img\n","        else:\n","            file_name = lr_path.split('/')[-1]\n","            if transforms is not None:\n","                transformed = self.transforms(image=lr_img)\n","                lr_img = transformed['image'] / 255.\n","            return lr_img, file_name\n","        \n","    def __len__(self):\n","        return len(self.df)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LIl8iPWyktd8","outputId":"64675ad3-4ed8-4167-dfe9-562d22bc856a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/functional.py:478: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2895.)\n","  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"]}],"source":["WRAPPED_MODEL = xmp.MpModelWrapper(SwinIR(upscale=4, in_chans=1, img_size=32, window_size=8,\n","                    img_range=1., depths=[6, 6, 6, 6], embed_dim=60, num_heads=[6, 6, 6, 6],\n","                    mlp_ratio=2, upsampler='pixelshuffledirect', resi_connection='1conv'))\n","\n","def map_fn(index, flags):\n","  ## Setup \n","\n","  # Sets a common random seed - both for initialization and ensuring graph is the same\n","  torch.manual_seed(flags['seed'])\n","\n","  my_transform = A.Compose([\n","        ToTensorV2(p=1.0)],\n","        additional_targets={'image': 'image', 'label': 'image'}\n","    )\n","\n","  # Downloads train and test datasets\n","  # Note: master goes first and downloads the dataset only once (xm.rendezvous)\n","  #   all the other workers wait for the master to be done downloading.\n","  train_dataset = CustomDataset(train_df, my_transform, True)\n","  val_dataset = CustomDataset(val_df, my_transform, True)\n","  \n","  # Creates the (distributed) train sampler, which let this process only access\n","  # its portion of the training dataset.\n","  train_sampler = torch.utils.data.distributed.DistributedSampler(\n","    train_dataset,\n","    num_replicas=xm.xrt_world_size(),\n","    rank=xm.get_ordinal(),\n","    shuffle=True)\n","  \n","  val_sampler = torch.utils.data.distributed.DistributedSampler(\n","    val_dataset,\n","    num_replicas=xm.xrt_world_size(),\n","    rank=xm.get_ordinal(),\n","    shuffle=False)\n","  \n","  # Creates dataloaders, which load data in batches\n","  # Note: test loader is not shuffled or sampled\n","  \n","  train_loader = torch.utils.data.DataLoader(\n","      train_dataset,\n","      batch_size=flags['batch_size'],\n","      sampler=train_sampler,\n","      num_workers=flags['num_workers'],\n","      drop_last=True)\n","\n","  val_loader = torch.utils.data.DataLoader(\n","      val_dataset,\n","      batch_size=flags['batch_size'],\n","      # batch_size=2,\n","      sampler=val_sampler,\n","      shuffle=False,\n","      num_workers=flags['num_workers'],\n","      drop_last=True)\n","  \n","\n","  ## Network, optimizer, and loss function creation\n","\n","  # Creates AlexNet for 10 classes\n","  # Note: each process has its own identical copy of the model\n","  #  Even though each model is created independently, they're also\n","  #  created in the same way.\n","  # Acquires the (unique) Cloud TPU core corresponding to this process's index\n","  device = xm.xla_device()\n","  net = WRAPPED_MODEL.to(device)\n","  xm.rendezvous('model')\n","  loss_fn = nn.L1Loss()\n","  optimizer = torch.optim.Adam(params = net.parameters(), lr = flags['learning_rate'])\n","  scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n","                                        lr_lambda=lambda epoch: 0.95 ** epoch)\n","\n","  ## Trains\n","  train_start = time.time()\n","  for epoch in range(flags['epochs']):\n","    train_loss = 0\n","    para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n","    para_val_loader = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n","    net.train()\n","    for batch_num, batch in enumerate(para_train_loader):\n","      lr_img, hr_img = batch[0].float().to(device), batch[1].float().to(device)\n","      # Acquires the network's best guesses at each class\n","      output = net(lr_img)\n","\n","      # Computes loss\n","      loss = loss_fn(output, hr_img)\n","      \n","      # Updates model\n","      optimizer.zero_grad()\n","      loss.backward()\n","      train_loss += loss.item()    \n","      # Note: optimizer_step uses the implicit Cloud TPU context to\n","      #  coordinate and synchronize gradient updates across processes.\n","      #  This means that each process's network has the same weights after\n","      #  this is called.\n","      # Warning: this coordination requires the actions performed in each \n","      #  process are the same. In more technical terms, the graph that\n","      #  PyTorch/XLA generates must be the same across processes. \n","      xm.optimizer_step(optimizer)  # Note: barrier=True not needed when using ParallelLoader\n","\n","    scheduler.step()  \n","    val_loss = 0.0\n","    net.eval()\n","    with torch.no_grad():\n","      for lr_img, hr_img in val_loader:\n","        lr_img, hr_img = batch[0].float().to(device), batch[1].float().to(device)\n","        predicted_outputs = net(lr_img)\n","        val_loss += loss_fn(predicted_outputs, hr_img)\n","\n","      print(f'Epoch : [{epoch}] Train Loss : [{train_loss/len(train_loader):.5f}], val Loss : [{val_loss/len(val_loader):.5f}], time: {time.time()-train_start}')\n","      torch.save(net.state_dict(), os.path.join(model_save_path, f'SwinIR_{str(epoch).zfill(4)}.pth'))\n","\n","  elapsed_train_time = time.time() - train_start\n","  print(\"Process\", index, \"finished training. Train time was:\", elapsed_train_time) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09pDI0ubczn7","outputId":"d33a8154-eae6-4ef5-ddd0-a52ddfc33e27"},"outputs":[{"name":"stderr","output_type":"stream","text":["Exception in device=TPU:0: Cannot replicate if number of devices (1) is different from 8\n","Traceback (most recent call last):\n","Exception in device=TPU:1: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","Traceback (most recent call last):\n","RuntimeError: Cannot replicate if number of devices (1) is different from 8\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","Exception in device=TPU:2: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","RuntimeError: Cannot replicate if number of devices (1) is different from 8\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","Exception in device=TPU:3: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","Traceback (most recent call last):\n","RuntimeError: Cannot replicate if number of devices (1) is different from 8\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","Exception in device=TPU:4: Cannot replicate if number of devices (1) is different from 8  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","\n","RuntimeError: Cannot replicate if number of devices (1) is different from 8\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","RuntimeError: Cannot replicate if number of devices (1) is different from 8\n","Exception in device=TPU:5: Cannot replicate if number of devices (1) is different from 8\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","RuntimeError: Cannot replicate if number of devices (1) is different from 8\n","Exception in device=TPU:6: Cannot replicate if number of devices (1) is different from 8\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 330, in _mp_start_fn\n","    _start_fn(index, pf_cfg, fn, args)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 323, in _start_fn\n","    _setup_replication()\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\", line 316, in _setup_replication\n","    xm.set_replication(device, [device])\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 318, in set_replication\n","    replication_devices = xla_replication_devices(devices)\n","  File \"/usr/local/lib/python3.7/dist-packages/torch_xla/core/xla_model.py\", line 288, in xla_replication_devices\n","    format(len(local_devices), len(kind_devices)))\n","Exception in device=TPU:7: Cannot replicate if number of devices (1) is different from 8\n"]},{"ename":"ProcessExitedException","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mProcessExitedException\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-95ef9bcf2645>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mxmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnprocs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'fork'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch_xla/distributed/xla_multiprocessing.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mjoin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mdaemon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdaemon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         start_method=start_method)\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m# Loop on join until it returns True or raises an exception.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/multiprocessing/spawn.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    152\u001b[0m                     \u001b[0merror_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                     \u001b[0merror_pid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfailed_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                     \u001b[0mexit_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexitcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m                 )\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mProcessExitedException\u001b[0m: process 0 terminated with exit code 17"]}],"source":["xmp.spawn(map_fn, args=(flags,), nprocs=flags['num_workers'], start_method='fork')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["mxCWWcG6LZ3A","IhQlqb3Q_x7P","SG5XqXvm_3ry","a-KONSkBB9n2","fSP4EaKYCH4A","Ice2f5G83bB_","arVx6CuTkXhG"],"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9184e31ca5324e198128f756b8ab5374":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_916b5566f987449fa0695638c47fce8e","IPY_MODEL_ef5f386085e347bebce5499887ad7383","IPY_MODEL_b02a2c091f9340899ce636ac8e785719"],"layout":"IPY_MODEL_fc663090d6b645039d744782ca16b98b"}},"916b5566f987449fa0695638c47fce8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc228504a14d4e3a9de76d4e439b454a","placeholder":"​","style":"IPY_MODEL_8f40e0dfbce9490eb5677b5d741e8227","value":"100%"}},"ef5f386085e347bebce5499887ad7383":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_17d65915dcb74338a74020b09f89d95a","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_00650a94aefb4a788884c378cf31f8e8","value":15}},"b02a2c091f9340899ce636ac8e785719":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3dbb25379ec4fbf8bc80980e43454da","placeholder":"​","style":"IPY_MODEL_e1b18c46985143fb857fb1be3bbfa276","value":" 15/15 [00:39&lt;00:00,  2.25s/it]"}},"fc663090d6b645039d744782ca16b98b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc228504a14d4e3a9de76d4e439b454a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f40e0dfbce9490eb5677b5d741e8227":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"17d65915dcb74338a74020b09f89d95a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"00650a94aefb4a788884c378cf31f8e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f3dbb25379ec4fbf8bc80980e43454da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e1b18c46985143fb857fb1be3bbfa276":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5b96d0c48954c379639bbb295779e79":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e802c5e6ba074251a8d8d64144b66ae3","IPY_MODEL_9ff0e1ff1fc54a92bb63c55ca11c679e","IPY_MODEL_fec4f132d1ca4ced844d2124d5513ce7"],"layout":"IPY_MODEL_c06b3d1edf4647ae878daa466eb8ffad"}},"e802c5e6ba074251a8d8d64144b66ae3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c511a95af7414c208cd8729dceedc9aa","placeholder":"​","style":"IPY_MODEL_268501a219bf49b880a0d398b17f6926","value":"100%"}},"9ff0e1ff1fc54a92bb63c55ca11c679e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6a394b05808d4ca5853c376de4850207","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_26decb304c644199a605581104d76eab","value":3}},"fec4f132d1ca4ced844d2124d5513ce7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a6bd8df6643b4507a0e18f67579c36e4","placeholder":"​","style":"IPY_MODEL_8cf49f358eed40f1aebf55fd00397ee8","value":" 3/3 [00:03&lt;00:00,  1.08s/it]"}},"c06b3d1edf4647ae878daa466eb8ffad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c511a95af7414c208cd8729dceedc9aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"268501a219bf49b880a0d398b17f6926":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6a394b05808d4ca5853c376de4850207":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26decb304c644199a605581104d76eab":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a6bd8df6643b4507a0e18f67579c36e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8cf49f358eed40f1aebf55fd00397ee8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}